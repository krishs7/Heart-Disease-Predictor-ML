{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p0LI-OCjpuf9","executionInfo":{"status":"ok","timestamp":1745977276715,"user_tz":240,"elapsed":26585,"user":{"displayName":"Saikot Paul","userId":"02986314157298313686"}},"outputId":"30a05943-e57c-4ee8-8ae9-c3cb40322222"},"id":"p0LI-OCjpuf9","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","execution_count":null,"id":"13349c1d","metadata":{"id":"13349c1d","outputId":"ad24eafb-5936-4acd-d4cf-69784befd2cf","colab":{"base_uri":"https://localhost:8080/","height":383},"executionInfo":{"status":"error","timestamp":1745977279652,"user_tz":240,"elapsed":2874,"user":{"displayName":"Saikot Paul","userId":"02986314157298313686"}}},"outputs":[{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'SimpleImputers' from 'sklearn.impute' (/usr/local/lib/python3.11/dist-packages/sklearn/impute/__init__.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-c5dab878cd26>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleImputers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'SimpleImputers' from 'sklearn.impute' (/usr/local/lib/python3.11/dist-packages/sklearn/impute/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import numpy as np\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import log_loss\n","from sklearn.impute import SimpleImputers\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import log_loss, accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n","import pandas as pd\n","\n","class LogisticRegr:\n","    def __init__(self):\n","        self.model = LogisticRegression(penalty=\"l2\", solver='liblinear')\n","        self.flag = False\n","        self.flag_y_pred = None\n","\n","    def fit(self, X, y):\n","        y_unique = np.unique(y)\n","        if len(y_unique) == 1:\n","            self.flag = True\n","            self.flag_y_pred = y_unique[0]\n","        else:\n","            self.model.fit(X, y)\n","\n","    def predict(self, X):\n","        if self.flag:\n","            return np.full(len(X), self.flag_y_pred, dtype=int)\n","        else:\n","            return self.model.predict(X)\n","\n","    def predict_proba(self, X):\n","        X = np.array(X)\n","        if X.ndim == 1:\n","            X = X.reshape(1, -1)\n","\n","        if self.flag:\n","            proba = np.zeros((len(X), 2))\n","            proba[:, int(self.flag_y_pred)] = 1.0\n","            return proba\n","        else:\n","            return self.model.predict_proba(X)[:, 1]\n","\n","    def loss(self, y, y_pred):\n","        return log_loss(y, y_pred, labels=[0,1])\n","\n","\n","class ModelTree:\n","    def __init__(self, model, max_depth=5, min_samples_leaf=10, verbose=True):\n","        self.model = model\n","        self.max_depth = max_depth\n","        self.min_samples_leaf = min_samples_leaf\n","        self.verbose = verbose\n","        self.tree = None\n","\n","    def fit(self, X, y):\n","        self.tree = self._build_tree(X, y)\n","        return self.tree\n","\n","    def predict(self, X):\n","        assert self.tree is not None, \"ModelTree not fitted\"\n","        return np.array([self._predict_node(self.tree, x) for x in X])\n","\n","    def predict_proba(self, X):\n","        assert self.tree is not None, \"ModelTree not fitted\"\n","        return np.array([self._proba_node(self.tree, x) for x in X])\n","\n","    def loss(self, y, y_pred):\n","        return self.model().loss(y, y_pred)\n","\n","    def _build_tree(self, X, y, depth=0, container=None):\n","        if container is None:\n","            container = {\"index_node_global\": 0}\n","\n","        node = self._create_node(X, y, depth, container)\n","\n","        if depth >= self.max_depth or len(X) <= self.min_samples_leaf:\n","            if self.verbose:\n","                self._print_node(node, depth)\n","            return node\n","\n","        split_result = self._split_node(node)\n","\n","        if not split_result[\"did_split\"]:\n","            if self.verbose:\n","                self._print_node(node, depth)\n","            return node\n","\n","        node.update({\n","            \"j_feature\": split_result[\"j_feature\"],\n","            \"threshold\": split_result[\"threshold\"],\n","            \"children\": {\n","                \"left\": self._build_tree(*split_result[\"data\"][0], depth + 1, container),\n","                \"right\": self._build_tree(*split_result[\"data\"][1], depth + 1, container)\n","            }\n","        })\n","\n","        return node\n","\n","    def _fit_model(self, X, y):\n","        model = self.model()\n","        model.fit(X, y)\n","        preds = model.predict(X)\n","        loss = model.loss(y, preds)\n","\n","        return loss, model\n","\n","    def _create_node(self, X, y, depth, container):\n","        loss, model = self._fit_model(X, y)\n","\n","        node = {\n","            'name': 'node',\n","            'index': container['index_node_global'],\n","            'loss': loss,\n","            'model': model,\n","            'data': (X, y),\n","            'n_samples': len(X),\n","            'j_feature': None,\n","            'threshold': None,\n","            'children': {\n","                'left': None,\n","                'right': None\n","            },\n","            'depth': depth\n","        }\n","\n","        container[\"index_node_global\"] += 1\n","        return node\n","\n","    def _split_data(self, feature, X, y, threshold):\n","        left_idx = np.where(X[:, feature] <= threshold)[0]\n","        right_idx = np.setdiff1d(np.arange(len(X)), left_idx)\n","\n","        left_X, left_y = X[left_idx], y[left_idx]\n","        right_X, right_y = X[right_idx], y[right_idx]\n","\n","        return left_X, left_y, right_X, right_y\n","\n","    def _split_node(self, node):\n","        X, y = node[\"data\"]\n","        depth = node[\"depth\"]\n","        N, d = X.shape\n","\n","        best_split = {\n","            \"did_split\": False,\n","            \"loss\": node[\"loss\"],\n","            \"data\": None,\n","            \"j_feature\": None,\n","            \"threshold\": None,\n","            \"N\": N\n","        }\n","\n","        if depth >= self.max_depth:\n","            return best_split\n","\n","        for j_feature in range(d):\n","            thresholds = np.unique(X[:, j_feature]) # Get all unique values\n","\n","            for threshold in thresholds:\n","                X_left, y_left, X_right, y_right = self._split_data(j_feature, X, y, threshold)\n","\n","                if (len(X_left) < self.min_samples_leaf or len(X_right) < self.min_samples_leaf):\n","                    continue\n","\n","                left_loss, left_model = self._fit_model(X_left, y_left)\n","                right_loss, right_model = self._fit_model(X_right, y_right)\n","                loss_split = (len(X_left)*left_loss + len(X_right)*right_loss) / N\n","\n","                if loss_split < best_split[\"loss\"]:\n","                    best_split.update({\n","                        \"did_split\": True,\n","                        \"loss\": loss_split,\n","                        \"data\": [(X_left, y_left), (X_right, y_right)],\n","                        \"j_feature\": j_feature,\n","                        \"threshold\": threshold,\n","                    })\n","\n","        return best_split\n","\n","    def _predict_node(self, node, x):\n","        if node[\"children\"][\"left\"] is None and node[\"children\"][\"right\"] is None:\n","            return node[\"model\"].predict(x.reshape(1, -1))[0]\n","\n","        feature = node[\"j_feature\"]\n","        threshold = node[\"threshold\"]\n","\n","        if x[feature] < threshold:\n","            return self._predict_node(node[\"children\"][\"left\"], x)\n","        else:\n","            return self._predict_node(node[\"children\"][\"right\"], x)\n","\n","    def _proba_node(self, node, x):\n","        if node[\"children\"][\"left\"] is None and node[\"children\"][\"right\"] is None:\n","            return node[\"model\"].predict_proba(x.reshape(1, -1))[0]\n","\n","        feature = node[\"j_feature\"]\n","        threshold = node[\"threshold\"]\n","\n","        if x[feature] < threshold:\n","            return self._proba_node(node[\"children\"][\"left\"], x)\n","        else:\n","            return self._proba_node(node[\"children\"][\"right\"], x)\n","\n","    def _print_node(self, node, depth):\n","        print(f\"Depth: {depth}, Node Index: {node['index']}, Loss: {node['loss']}, Samples: {node['n_samples']}\")\n","\n","# Load dataset\n","dataset = pd.read_csv('/content/drive/MyDrive/CAPSTONE/Colab/Code/Code/cleve.csv')\n","X = dataset.iloc[:, :-1].values\n","y = dataset.iloc[:, -1].values  # Assuming the target variable is in the last column\n","\n","# Handle missing data\n","imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n","X[:, 11:13] = imputer.fit_transform(X[:, 11:13])\n","\n","# Splitting the dataset\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=9)\n","\n","# Feature Scaling\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test)\n","\n","\n","model = LogisticRegr\n","tree = ModelTree(model=model, max_depth=10)\n","tree.fit(X_train, y_train)\n","predictions = tree.predict(X_test)"]},{"cell_type":"code","execution_count":null,"id":"e7663e6a","metadata":{"id":"e7663e6a"},"outputs":[],"source":["from sklearn.utils import resample\n","\n","class ModelForest:\n","    def __init__(self, model, n_trees=10, max_depth=5, min_samples_leaf=10, verbose=True, max_features=\"sqrt\"):\n","        self.model = model\n","        self.n_trees = n_trees\n","        self.max_depth = max_depth\n","        self.min_samples_leaf = min_samples_leaf\n","        self.verbose = verbose\n","        self.max_features = max_features\n","        self.trees = []\n","\n","    def fit(self, X, y):\n","        n_samples, n_features = X.shape\n","\n","        if self.max_features == \"sqrt\":\n","            self.n_features = int(np.sqrt(n_features))\n","        elif self.max_features == \"log2\":\n","            self.n_features = int(np.log2(n_features))\n","        elif isinstance(self.max_features, int):\n","            self.n_features = self.max_features\n","        else:\n","            raise ValueError(\"max_features must be 'sqrt', 'log2', or an integer.\")\n","\n","        self.trees = []\n","        for i in range(self.n_trees):\n","            if self.verbose:\n","                print(f\"Building tree {i + 1}/{self.n_trees}\")\n","\n","            X_sample, y_sample = resample(X, y, replace=True)\n","\n","            tree = ModelTree(\n","                model=self.model,\n","                max_depth=self.max_depth,\n","                min_samples_leaf=self.min_samples_leaf,\n","                verbose=self.verbose\n","            )\n","            tree.fit(X_sample, y_sample)\n","            self.trees.append(tree)\n","\n","    def predict(self, X):\n","        predictions = np.array([tree.predict(X) for tree in self.trees])\n","        return np.round(np.mean(predictions, axis=0)).astype(int)  # Majority voting\n","\n","    def predict_proba(self, X):\n","        probas = np.array([tree.predict_proba(X) for tree in self.trees])\n","        return np.mean(probas, axis=0)  # Average probabilities\n","\n","    def loss(self, y, y_pred):\n","        return log_loss(y, y_pred, labels=[0, 1])"]},{"cell_type":"code","execution_count":null,"id":"b26ca7f4","metadata":{"id":"b26ca7f4"},"outputs":[],"source":["tree = ModelTree(LogisticRegr, min_samples_leaf=10)\n","tree.fit(X_train, y_train)\n","preds = tree.predict(X_test)\n","print(classification_report(y_test, preds))"]},{"cell_type":"code","execution_count":null,"id":"ebcf43e4","metadata":{"id":"ebcf43e4"},"outputs":[],"source":["forest = ModelForest(LogisticRegr, min_samples_leaf=10)\n","forest.fit(X_train, y_train)\n","preds = forest.predict(X_test)\n","print(classification_report(y_test, preds))"]},{"cell_type":"code","execution_count":null,"id":"cca3c611","metadata":{"id":"cca3c611"},"outputs":[],"source":["forest = ModelForest(LogisticRegr, n_trees=20, min_samples_leaf=5)\n","forest.fit(X_train, y_train)\n","preds = forest.predict(X_test)\n","print(classification_report(y_test, preds))"]},{"cell_type":"code","execution_count":null,"id":"2bf19c80","metadata":{"id":"2bf19c80"},"outputs":[],"source":["forest = ModelForest(LogisticRegr, n_trees=20, min_samples_leaf=2)\n","forest.fit(X_train, y_train)\n","preds = forest.predict(X_test)\n","print(classification_report(y_test, preds))"]},{"cell_type":"code","execution_count":null,"id":"bba1630a","metadata":{"id":"bba1630a"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}